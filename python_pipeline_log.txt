NAME              IMAGE                                   COMMAND                  SERVICE           CREATED        STATUS          PORTS
debezium          debezium/connect:2.4                    "/docker-entrypoint.…"   debezium          30 hours ago   Up 31 minutes   0.0.0.0:8083->8083/tcp, [::]:8083->8083/tcp
kafka             confluentinc/cp-kafka:7.5.0             "/etc/confluent/dock…"   kafka             30 hours ago   Up 31 minutes   0.0.0.0:9092->9092/tcp, [::]:9092->9092/tcp
kafka-tools       confluentinc/cp-kafka:7.5.0             "/bin/sh"                kafka-tools       30 hours ago   Up 31 minutes   9092/tcp
pg_task1          postgres:15                             "docker-entrypoint.s…"   postgres          30 hours ago   Up 31 minutes   0.0.0.0:5433->5432/tcp, [::]:5433->5432/tcp
schema-registry   confluentinc/cp-schema-registry:7.5.0   "/etc/confluent/dock…"   schema-registry   30 hours ago   Up 31 minutes   0.0.0.0:8081->8081/tcp, [::]:8081->8081/tcp
spark             apache/spark:3.5.0                      "/opt/spark/bin/spar…"   spark             30 hours ago   Up 31 minutes   0.0.0.0:7077->7077/tcp, [::]:7077->7077/tcp, 0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp
spark-worker      apache/spark:3.5.0                      "/opt/spark/bin/spar…"   spark-worker      30 hours ago   Up 31 minutes   0.0.0.0:8082->8081/tcp, [::]:8082->8081/tcp
zookeeper         confluentinc/cp-zookeeper:7.5.0         "/etc/confluent/dock…"   zookeeper         30 hours ago   Up 31 minutes   2181/tcp, 2888/tcp, 3888/tcp
==================================================
Mini Data Platform Pipeline Execution
==================================================


[STEP 1] Checking environment configuration
--------------------------------------------------
.env file exists


[STEP 2] Starting Docker containers
--------------------------------------------------
Containers started. Waiting 15 seconds for services to initialize...


[STEP 3] Container status
--------------------------------------------------


[STEP 4] Waiting for PostgreSQL to be ready
--------------------------------------------------
Waiting for PostgreSQL to be ready... PostgreSQL is ready

PostgreSQL logs (last 10 lines):



[STEP 5] Waiting for Kafka to be ready
--------------------------------------------------
Waiting for Kafka to be ready... Kafka is ready

Kafka logs (last 10 lines):
[2025-11-24 20:31:21,415] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
[2025-11-24 20:31:21,415] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2025-11-24 20:36:21,415] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[2025-11-24 20:36:21,416] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2025-11-24 20:36:21,417] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
[2025-11-24 20:36:21,417] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2025-11-24 20:41:21,421] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[2025-11-24 20:41:21,422] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2025-11-24 20:41:21,423] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
[2025-11-24 20:41:21,423] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)



[STEP 6] Waiting for Schema Registry
--------------------------------------------------
Waiting for Schema Registry to be ready... Schema Registry is ready

Schema Registry logs (last 10 lines):
[2025-11-24 20:17:41,064] INFO 192.168.65.1 - - [24/Nov/2025:20:17:41 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 2 (io.confluent.rest-utils.requests)
[2025-11-24 20:17:44,713] INFO 192.168.65.1 - - [24/Nov/2025:20:17:44 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 5 (io.confluent.rest-utils.requests)
[2025-11-24 20:20:18,401] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-11-24 20:20:18,495] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-11-24 20:20:18,635] INFO [Producer clientId=producer-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-11-24 20:40:48,532] INFO 192.168.65.1 - - [24/Nov/2025:20:40:48 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 40 (io.confluent.rest-utils.requests)
[2025-11-24 20:42:12,347] INFO 192.168.65.1 - - [24/Nov/2025:20:42:12 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 5 (io.confluent.rest-utils.requests)
[2025-11-24 20:42:19,005] INFO 192.168.65.1 - - [24/Nov/2025:20:42:18 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 6 (io.confluent.rest-utils.requests)
[2025-11-24 20:42:27,744] INFO 192.168.65.1 - - [24/Nov/2025:20:42:27 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 6 (io.confluent.rest-utils.requests)
[2025-11-24 20:42:57,900] INFO 192.168.65.1 - - [24/Nov/2025:20:42:57 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/8.7.1" 4 (io.confluent.rest-utils.requests)



[STEP 7] Waiting for Debezium
--------------------------------------------------
Waiting for Debezium to be ready...Liczba plikow do przetworzenia: 3
Przetwarzam: data/data1.csv
Tworze tabele: data1
Wstawiam dane do: data1 (3 wierszy)
OK
Przetwarzam: data/data2.csv
Tworze tabele: data2
Wstawiam dane do: data2 (2 wierszy)
OK
Przetwarzam: data/data3.csv
Tworze tabele: data3
Wstawiam dane do: data3 (2 wierszy)
OK
Zakonczono przetwarzanie.
        List of relations
 Schema | Name  | Type  | Owner  
--------+-------+-------+--------
 public | data1 | table | pguser
 public | data2 | table | pguser
 public | data3 | table | pguser
(3 rows)

  key  | value  
-------+--------
 asdf  | ghjk
 satuk | gamma
 ble   | bleble
 asdf  | ghjk
 satuk | gamma
(5 rows)

HTTP/1.1 409 Conflict
Date: Mon, 24 Nov 2025 20:42:58 GMT
Content-Type: application/json
Content-Length: 74
Server: Jetty(9.4.51.v20230217)

{"error_code":409,"message":"Connector postgres-connector already exists"}__consumer_offsets
_schemas
connect_configs
connect_offsets
connect_statuses
pg.public.data1
pg.public.data2
pg.public.data3
 row_count | table_name 
-----------+------------
        12 | data1
         8 | data2
         8 | data3
(3 rows)

 Debezium is ready

Debezium logs (last 10 lines):
2025-11-24 20:42:13,000 INFO   ||  Successfully tested connection for jdbc:postgresql://postgres:5432/business_db with user 'pguser'   [io.debezium.connector.postgresql.PostgresConnector]
2025-11-24 20:42:13,003 INFO   ||  Connection gracefully closed   [io.debezium.jdbc.JdbcConnection]
2025-11-24 20:42:13,003 INFO   ||  AbstractConfig values: 
   [org.apache.kafka.common.config.AbstractConfig]
2025-11-24 20:42:13,004 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:12 +0000] "POST /connectors HTTP/1.1" 409 74 "-" "curl/8.7.1" 18   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:13,095 INFO   ||  7 records sent during previous 00:24:37.738, last recorded offset of {server=pg} partition is {transaction_id=null, lsn_proc=26854440, messageType=INSERT, lsn_commit=26854120, lsn=26854440, txId=750, ts_usec=1764016932786524}   [io.debezium.connector.common.BaseSourceTask]
2025-11-24 20:42:18,037 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:18 +0000] "GET /connectors/postgres-connector/status HTTP/1.1" 200 174 "-" "curl/8.7.1" 4   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:24,070 INFO   ||  WorkerSourceTask{id=postgres-connector-0} Committing offsets for 7 acknowledged messages   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2025-11-24 20:42:27,719 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:27 +0000] "GET /connectors/postgres-connector/status HTTP/1.1" 200 174 "-" "curl/8.7.1" 4   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:57,936 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:57 +0000] "GET /connectors HTTP/1.1" 200 22 "-" "curl/8.7.1" 2   [org.apache.kafka.connect.runtime.rest.RestServer]



[STEP 8] Executing Task 1: Loading CSV data into PostgreSQL
--------------------------------------------------
Running: python3 business_project_task_1.py
--------------------------------------------------
--------------------------------------------------
Task 1 completed


[STEP 9] Verifying data in PostgreSQL
--------------------------------------------------
Tables created:

Sample data from data1:


[STEP 10] Registering Debezium PostgreSQL connector
--------------------------------------------------
POST http://localhost:8083/connectors
Payload:
{
  "name": "postgres-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "tasks.max": "1",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "pguser",
    "database.password": "admin",
    "database.dbname": "business_db",
    "database.server.name": "pgserver",
    "topic.prefix": "pg",
    "schema.include.list": "public",
    "table.include.list": "public.*",
    "plugin.name": "pgoutput",
    "slot.name": "debezium_slot",
    "publication.autocreate.mode": "all_tables"
  }
}

Response:

Waiting 5 seconds for connector to initialize...


[STEP 11] Debezium connector status
--------------------------------------------------
{
  "name": "postgres-connector",
  "connector": {
    "state": "RUNNING",
    "worker_id": "172.18.0.8:8083"
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "172.18.0.8:8083"
    }
  ],
  "type": "source"
}

Debezium logs after connector registration (last 20 lines):
2025-11-24 20:41:17,664 INFO   ||  [AdminClient clientId=1--shared-admin] Node 1 disconnected.   [org.apache.kafka.clients.NetworkClient]
2025-11-24 20:42:12,392 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:12 +0000] "GET /connectors HTTP/1.1" 200 22 "-" "curl/8.7.1" 8   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:12,992 INFO   ||  Loading the custom source info struct maker plugin: io.debezium.connector.postgresql.PostgresSourceInfoStructMaker   [io.debezium.config.CommonConnectorConfig]
2025-11-24 20:42:13,000 INFO   ||  Successfully tested connection for jdbc:postgresql://postgres:5432/business_db with user 'pguser'   [io.debezium.connector.postgresql.PostgresConnector]
2025-11-24 20:42:13,003 INFO   ||  Connection gracefully closed   [io.debezium.jdbc.JdbcConnection]
2025-11-24 20:42:13,003 INFO   ||  AbstractConfig values: 
   [org.apache.kafka.common.config.AbstractConfig]
2025-11-24 20:42:13,004 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:12 +0000] "POST /connectors HTTP/1.1" 409 74 "-" "curl/8.7.1" 18   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:13,095 INFO   ||  7 records sent during previous 00:24:37.738, last recorded offset of {server=pg} partition is {transaction_id=null, lsn_proc=26854440, messageType=INSERT, lsn_commit=26854120, lsn=26854440, txId=750, ts_usec=1764016932786524}   [io.debezium.connector.common.BaseSourceTask]
2025-11-24 20:42:18,037 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:18 +0000] "GET /connectors/postgres-connector/status HTTP/1.1" 200 174 "-" "curl/8.7.1" 4   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:24,070 INFO   ||  WorkerSourceTask{id=postgres-connector-0} Committing offsets for 7 acknowledged messages   [org.apache.kafka.connect.runtime.WorkerSourceTask]
2025-11-24 20:42:27,719 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:27 +0000] "GET /connectors/postgres-connector/status HTTP/1.1" 200 174 "-" "curl/8.7.1" 4   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:57,936 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:57 +0000] "GET /connectors HTTP/1.1" 200 22 "-" "curl/8.7.1" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:42:58,551 INFO   ||  Loading the custom source info struct maker plugin: io.debezium.connector.postgresql.PostgresSourceInfoStructMaker   [io.debezium.config.CommonConnectorConfig]
2025-11-24 20:42:58,556 INFO   ||  Successfully tested connection for jdbc:postgresql://postgres:5432/business_db with user 'pguser'   [io.debezium.connector.postgresql.PostgresConnector]
2025-11-24 20:42:58,558 INFO   ||  Connection gracefully closed   [io.debezium.jdbc.JdbcConnection]
2025-11-24 20:42:58,558 INFO   ||  AbstractConfig values: 
   [org.apache.kafka.common.config.AbstractConfig]
2025-11-24 20:42:58,559 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:42:58 +0000] "POST /connectors HTTP/1.1" 409 74 "-" "curl/8.7.1" 11   [org.apache.kafka.connect.runtime.rest.RestServer]
2025-11-24 20:43:03,600 INFO   ||  192.168.65.1 - - [24/Nov/2025:20:43:03 +0000] "GET /connectors/postgres-connector/status HTTP/1.1" 200 174 "-" "curl/8.7.1" 4   [org.apache.kafka.connect.runtime.rest.RestServer]



[STEP 13] Kafka topics created by Debezium
--------------------------------------------------


[STEP 14] Schema Registry - Registered schemas
--------------------------------------------------
No schemas registered yet (will appear after first CDC event)


[STEP 15] Checking Kafka message counts
--------------------------------------------------
  pg.public.data1: 12 messages
  pg.public.data2: 8 messages
  pg.public.data3: 8 messages


[STEP 16] Testing AVRO consumer (5 seconds)
--------------------------------------------------
Running: python3 consumer_avro.py
--------------------------------------------------
Consumer test timed out after 5 seconds
--------------------------------------------------


[STEP 17] Spark cluster status
--------------------------------------------------
Spark master: Running
Spark UI: http://localhost:8080

Spark master logs (last 10 lines):


Spark worker: Running
Spark worker logs (last 10 lines):



[STEP 18] Pipeline State Summary
--------------------------------------------------
--------------------------------------------------
PostgreSQL:

Kafka Topics:
  - pg.public.data1
  - pg.public.data2
  - pg.public.data3

Debezium Connector:
  Status: RUNNING

Schema Registry:
  Registered schemas: 0


[STEP 19] Next Steps
--------------------------------------------------
--------------------------------------------------
1. Generate CDC events by inserting data:
   docker exec -it pg_task1 psql -U pguser -d business_db -c "INSERT INTO data1 (key, value) VALUES ('test', 'value');"

2. Run Spark job to process stream:
   ./run_spark_job.sh

3. Monitor Spark UI:
   http://localhost:8080

4. View real-time logs:
   docker logs -f <container-name>

==================================================
Pipeline execution completed!
==================================================
